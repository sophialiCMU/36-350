---
title: "HW: Week 9"
author: "36-350 -- Statistical Computing"
date: "Week 9 -- Spring 2022"
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: space
---

Name: Sophia Li

Andrew ID: sophiali


You must submit **your own** HW as a PDF file on Gradescope.

---

```{r wrap-hook,echo=FALSE}
library(knitr)
library(MASS)
library(tidyverse)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  knitr::opts_chunk$set(linewidth = 80)
    # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```


## HW Length Cap Instructions
* If the question requires you to print a data frame in your solution e.g. `q1_out_df`, you must first apply **head(q1_out_df, 30)** and **dim(q1_out_df)** in the final knitted pdf output for such a data frame.
* Please note that this only applies if you are knitting the `Rmd` to a `pdf`, for Gradescope submission purposes.
* If you are using the data frame output for visualization purposes (for example), use the entire data frame in your exploration
* The **maximum allowable length** of knitted pdf HW submission is **30 pages**. Submissions exceeding this length *will not be graded* by the TAs. All pages must be tagged as usual for the required questions per the usual policy
* For any concerns about HW length for submission, please reach out on Piazza during office hours


## Question 1
*(20 points)*

Display the sampling distribution for $R_{1,2}$, the off-diagonal element of a two-dimensional sample correlation matrix for a bivariate normal, given that $\mu_1 = \mu_2 = 2$, $\sigma_1 = 1$, $\sigma_2 = 2$, $\rho_{1,2} = -0.5$, and $n = 100$. Sample 1000 values and display them in a histogram; include a vertical line for the population value ($-0.5$). (Reminder: if I don't specify how exactly to visualize something, i.e., if I don't specify base `R` versus `ggplot`, then you can choose whichever.) Note that the final distribution that you see will not be normal.
```{r}
# Notes 9A slide 5/11
mu = c(2,2)
sigma.1 = 1
sigma.2 = 2
rho.12 = -0.5
n = 100

Sigma = matrix(c(sigma.1^2,
                 rho.12*sigma.1*sigma.2, 
                 rho.12*sigma.1*sigma.2,
                 sigma.2^2), nrow = 2)

set.seed(101)
data = rep(0,1000)
fun = function(x) {
  d = mvrnorm(100,mu,Sigma)
  cor(d)[1,2]
}

# graph it
data = sapply(data, FUN=fun)
hist(data, col="darkturquoise")
abline(v = -0.5, col = "forestgreen", lwd=5, lty=2)
```

## Question 2
*(20 points)*

Assume you have a trivariate multivariate normal with $\mu = \{2,3,4\}$ and $\sigma = \{1,2,1\}$, and $\rho_{1,2} = 0.4$, $\rho_{1,3} = 0.7$, and $\rho_{2,3} = -0.2$. Sample 1000 data from the marginal distribution for $(x_1,x_3)$, and display them via `ggplot`. Compute the sample means along each marginal axis: they should be approximately 2 and 4.
```{r}
set.seed(101)
mu = c(2,3,4)
sigma = c(1,2,1)
rho.12 = 0.4
rho.13 = 0.7
rho.23 = -0.2

rho = matrix(c(1,rho.12, rho.13,
               rho.12, 1, rho.23,
               rho.13, rho.23, 1), 3,3)
Sigma = rho * (sigma %o% sigma)

# Get data
mu.marg = mu[c(1,3)]
Sigma.marg = Sigma[c(1,3), c(1,3)]
data = mvrnorm(1000,mu.marg,Sigma.marg)

# Plot
df = data.frame(data)
names(df) = c("x1", "x3")
ggplot(data=df,mapping=aes(x=x1,y=x3)) + geom_point(col="skyblue")

# get means
mean(df$x1)
mean(df$x3)

```

## Question 3
*(20 points)*

Repeat Q2, except here you should sample from the conditional distribution $f(x_1,x_3 \vert x_2 = 1)$. Compute the conditional correlation coefficient $\rho_{\rm cond}$ between the data along the $x_1$ and $x_3$ axes, given $x_2 = 1$, and display the sample correlation matrix.
```{r}
set.seed(202)

# Get data
k = c(1,3)
d.minus.k = c(2)
Sigma.kk = Sigma[k, k]
Sigma.kd = Sigma[k, d.minus.k]
Sigma.dk = Sigma[d.minus.k, k]
Sigma.dd = Sigma[d.minus.k, d.minus.k]

mu.cond = mu[k] + Sigma.kd %*% solve(Sigma.dd) %*% matrix(c(1)-mu[d.minus.k])

Sigma.cond = Sigma.kk - Sigma.kd %*% solve(Sigma.dd) %*% Sigma.dk

# Sample Data
mu.marg = mu.cond[c(1,2)]
Sigma.marg = Sigma.cond[c(1,2), c(1,2)]
data = mvrnorm(1000, mu.marg,Sigma.marg)

# Plot
df = data.frame(data)
names(df) = c("x1", "x3")
ggplot(data=df,mapping=aes(x=x1,y=x3)) + geom_point(col="skyblue")

cor(df)
# conditional correlation coefficient Q**
# it is 
# should be 0.868599
Sigma.cond
Sigma.cond = Sigma.kk - Sigma.kd %*% solve(Sigma.dd) %*% Sigma.dk
# constructed from Sigma.cond. Use matrix to compute correlation coeffient
```

## Question 4
*(20 points)*

Assume that you have a mixture model: you have 100 data sampled from a bivariate normal with $\mu = \{1,1\}$ and $\sigma = \{1.2,1.2\}$, with $\rho = 0.4$, and another 100 data sampled from a bivariate normal with $\mu = \{3,3\}$, $\sigma = \{1,1\}$, and $\rho = -0.6$. Plot your sampled data with separate colors for each component of the mixture. Then perform logistic regression to try to classify each sampled point as being from component 1 or component 2, and output the proportion of times you misclassify a point. (Don't worry about breaking your data up into training and testing sets, as this is a simple academic exercise; just use all 200 points to train your classifier, then output the training misclassification error.)

How to train your logistic classifier and get the misclassification rate?

- Assuming you already have a data frame with sampled $x_1$ and $x_2$ values in the first and second columns, add a third column with the labeled class. (Name this column `class`.) Use 0 for the first class, and 1 for the second.
- Use `glm()` with model formula `class~.`, your data frame, and the argument `family=binomial`.
- Use `predict()` with the output of `glm()` and with the argument `type="response"`. This will generate 200 predictions between 0 and 1.
- Round off all predictions to 0 or 1.
- Create a `table()` with the arguments being your rounded-off predictions and the labeled classes.
- Compute the proportion of table elements that are "off-diagonal" (upper-right and lower-left). Done.
```{r}
set.seed(303)
mu = c(1,1)
sigma.1 = sigma.2 = 1.2
sigma = c(1.2, 1.2)
rho.12 = 0.4
Sigma = matrix(c(sigma.1^2, rho.12*sigma.1*sigma.2, 
                 rho.12*sigma.1*sigma.2, sigma.2^2), nrow = 2)
data1 = data.frame(mvrnorm(100,mu,Sigma))

# sampled from
mu = c(3,3)
sigma.1 = sigma.2 = 1
sigma = c(1,1)
rho.12 = -0.6
Sigma = matrix(c(sigma.1^2,  rho.12*sigma.1*sigma.2, 
                 rho.12*sigma.1*sigma.2, sigma.2^2), nrow = 2)
data2 = data.frame(mvrnorm(100,mu,Sigma))

## Plot
ggplot(data1, aes(x=X1, y=X2)) +
  geom_point(data1, mapping = aes(x=X1, y=X2), color="firebrick") +
  geom_point(data2, mapping = aes(x=X1, y=X2), color="darkolivegreen3")

## Train and Classify
training = rbind(data1, data2)
training["class"] = c(rep(0,100), rep(1,100))
out = glm(formula=class~., data=training, family=binomial)
p = predict(out, type="response")
df = data.frame(round(p))
names(df) = "predicted"

# print table
t = table(training$class, round(p))
(t[2] + t[3])/sum(t)
# t = cbind(actual = training$class, df)
# sum(t$actual != t$predicted)/200
```

---

In the following code chunk, we input seven measurements for each of 5000 asteroids. The data frame is `df`. There is another variable, `q`, that is also loaded and which we will utilize later.
```{r linewidth=80}
load(url("http://www.stat.cmu.edu/~mfarag/350/HW_09_PCA.Rdata"))
names(df)
```

---

## Question 5
*(20 points)*

Perform PCA on the data frame `df`. Use the rule-of-thumb in the notes to determine the number of principal components (or PCs) to retain, and for those PCs, indicate the mapping from PC to original variables. (Also, display the proportion of variance explained by the PCs you retain.) Are any of the original variables "unimportant" within the context of the retained PCs?
```{r linewidth=80}
# rule of thumb = pick the smallest # of PCs that explain greater than 90% of the variance
# p$sdev[PC]^2/sum(p$sdev^2)
# PC1 = 0.3352716, PC2 = 0.2615384, PC3 = 0.145932,
# PC4 = 0.1234097, PC5 = 0.1076712, PC6 = 0.2251361,
# PC7 = 0.003463479
# We will need the first 5 PCs to get to over 90% (97.40%)
# Got it working. I looked at p$scale to get the scaled values. albedo, e, and H are the lowest values at 0.06640138, #0.08106676, and 1.43902338.



p = prcomp(df, scale=TRUE)
plot(1:7,cumsum(p$sdev^2)/sum(p$sdev^2),pch=19,xlab="PC",ylab="Cumulative Explained Variance",ylim=c(0,1))
abline(h=0.9,lty=2)
c = c(1,2,3,4,5)
s = sum(p$sdev[c]^2)/sum(p$sdev^2)

df_new = data.frame(p$rotation)[c]
df_new
```
```

a influences PC1 the most, H influences PC2, \n
albendo influcences PC2, e influences PC4, i influences PC5. \n
The variables that don't greatly influence any PC are \n
diameter and per_y.


```

## Question 6
*(20 points)*

Something that one can do with principal components is regression: after all, all you've done is transform your data to a new coordinate system. Below, linearly regress the variable `q` upon all the variables in `df`, and print the adjusted $R^2$ and the sum of squared errors for the model. Then repeat linear regression, except now regress the variable `q` upon only the retained PCs. Again, print out the adjusted $R^2$ and the sum of squared errors. Are the second value close to the first? (They often will be, but don't have to be.) (Hint: look at the names of the list elements that are output by the *summary* of your linear regression fits, as one of those list elements may help you with extracting the adjusted $R^2$. As far as the sum of squared errors, you need to simply compute the `sum()` of the difference between the observed values of `q` and the predicted values from `predict()`.)
```{r linewidth=80}
# df_save = df
# out = glm(formula=class~., data=training, family=binomial)
# ## Try stuff
# scale = scale(df)%*%p$rotation[, c]
# model1 = lm(q~., data = data.frame(scale))
# sum(model1$coefficients)
# sum(model1$coefficients)
# sum(unlist(model1))

# (R2 = 0.7390329)
# (Square = 2469.368)
# (AdjR2 = 0.1842183)
# (Square2 = 7722.323)
```
```
I tried. I'll just take the F for this problem.
```